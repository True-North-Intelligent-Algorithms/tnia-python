{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIY Interactive Segmentation with napari\n",
    "\n",
    "This is Kyle Harrington's tutorial adapted from [here](https://github.com/kephale/napari-dl-at-mbl-2024/blob/main/napari-workshops/notebooks/diy_interactive_segmentation.ipynb) adapted for the SEM grain images shared [here](https://forum.image.sc/t/free-article-from-machine-learning-to-deep-learning-revolutionizing-microscopy-image-analysis/106340/3)\n",
    "\n",
    "BN:  I added in live instance segmentation so we can see if the objects in the image are separated. \n",
    "\n",
    "+++\n",
    "\n",
    "napari is a very flexible and \"hackable\" tool. In this tutorial we will\n",
    "make a custom interactive segmentation tool from scratch.\n",
    "\n",
    "+++\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from appdirs import user_data_dir\n",
    "import os\n",
    "import zarr\n",
    "import dask.array as da\n",
    "import toolz as tz\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from skimage import data, segmentation, feature, future\n",
    "from skimage.feature import multiscale_basic_features\n",
    "from skimage.io import imread, imshow\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import napari\n",
    "import threading\n",
    "\n",
    "from ome_zarr.io import parse_url\n",
    "from ome_zarr.reader import Reader\n",
    "\n",
    "from functools import partial\n",
    "from psygnal import debounced\n",
    "from superqt import ensure_main_thread\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from skimage.measure import label\n",
    "\n",
    "LOGGER = logging.getLogger(\"halfway_to_i2k_2023_america\")\n",
    "LOGGER.setLevel(logging.DEBUG)\n",
    "\n",
    "streamHandler = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter(\n",
    "    \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "streamHandler.setFormatter(formatter)\n",
    "LOGGER.addHandler(streamHandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open the data\n",
    "\n",
    "For simplicity we will just use the first channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "#tnia_images_path = Path(r\"D:/images/tnia-python-images\")\n",
    "tnia_images_path = Path(r\"/home/bnorthan/images/tnia-python-images/\")\n",
    "\n",
    "parent_path = tnia_images_path / \"imagesc/2024_12_19_sem_grain_size_revisit\"\n",
    "image_name = \"211122_AM_Al2O3_SE_021\"\n",
    "image = imread(str(parent_path / (\"211122_AM_Al2O3_SE_021.ome.tiff\")))\n",
    "\n",
    "prediction_name = image_name+\"_prediction_cells\"\n",
    "painting_name = image_name+\"painting_cells\"\n",
    "instance_name = image_name+\"instance_cells\"\n",
    "instance_painting_name = image_name+\"instance_painting_cells\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize in Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()\n",
    "\n",
    "data_layer = viewer.add_image(image)\n",
    "data_layer.bounding_box.visible = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image, feature_params):\n",
    "    features_func = partial(\n",
    "        multiscale_basic_features,\n",
    "        intensity=feature_params[\"intensity\"],\n",
    "        edges=feature_params[\"edges\"],\n",
    "        texture=feature_params[\"texture\"],\n",
    "        sigma_min=feature_params[\"sigma_min\"],\n",
    "        sigma_max=feature_params[\"sigma_max\"],\n",
    "        channel_axis=None,\n",
    "    )\n",
    "    # print(f\"image shape {image.shape} feature params {feature_params}\")\n",
    "\n",
    "    if len(image.shape) == 2:\n",
    "        features = features_func(image)\n",
    "        return features\n",
    "    elif len(image.shape) == 3:\n",
    "        \n",
    "        for c in range(image.shape[-1]):\n",
    "            features_temp = features_func(np.squeeze(image[..., c]))\n",
    "            if c == 0:\n",
    "                features = features_temp\n",
    "            else:\n",
    "                features = np.concatenate((features, features_temp), axis=2)\n",
    "        return features\n",
    "\n",
    "example_feature_params = {\n",
    "    \"sigma_min\": 1,\n",
    "    \"sigma_max\": 5,\n",
    "    \"intensity\": True,\n",
    "    \"edges\": True,\n",
    "    \"texture\": True,\n",
    "}\n",
    "\n",
    "\n",
    "features = extract_features(image, example_feature_params)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_features():\n",
    "    for feature_idx in range(features.shape[-1]):\n",
    "        viewer.add_image(features[..., feature_idx])\n",
    "        \n",
    "#show_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Interactive Segmentation Tool!\n",
    "\n",
    "Ok, now we've seen:\n",
    "- our data\n",
    "- some features we can compute for our data\n",
    "\n",
    "Our goal is to create an image where we have labels that correspond to the zebrafish sample. \n",
    "\n",
    "The approach is that when we annotate/draw in our painting layer, then we want our segmentations to be updated automatically. \n",
    "\n",
    "We will do this using 3 different image layers:\n",
    "\n",
    "1. Our input image\n",
    "2. A layer for painting\n",
    "3. A layer for storing the machine learning generated predictions\n",
    "\n",
    "Due to popular demand we will be using Zarr to store these layers, because that will help this approach scale to very large datasets. However, we could have used numpy arrays as well.\n",
    "\n",
    "+++\n",
    "\n",
    "### Create our painting and prediction layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_path = r\"./zarr\"\n",
    "print(f\"Saving outputs to zarr path: {zarr_path}\")\n",
    "\n",
    "# Create a prediction layer\n",
    "prediction_data = zarr.open(\n",
    "    f\"{zarr_path}/{prediction_name}\",\n",
    "    mode='a',\n",
    "    shape=image.shape[:2],\n",
    "    dtype='i4',\n",
    "    dimension_separator=\"/\",\n",
    "\n",
    ")\n",
    "prediction_layer = viewer.add_labels(prediction_data, name=\"Prediction\", scale=data_layer.scale)\n",
    "\n",
    "# Create a painting layer\n",
    "painting_data = zarr.open(\n",
    "    f\"{zarr_path}/{painting_name}\",\n",
    "    mode='a',\n",
    "    shape=image.shape[:2],\n",
    "    dtype='i4',\n",
    "    dimension_separator=\"/\",\n",
    ")\n",
    "\n",
    "painting_layer = viewer.add_labels(painting_data, name=\"Painting\", scale=data_layer.scale)\n",
    "\n",
    "# create a instance layer for painting\n",
    "instance_layer = zarr.open(\n",
    "    f\"{zarr_path}/{instance_name}\",\n",
    "    mode='a',\n",
    "    shape=image.shape[:2],\n",
    "    dtype='i4',\n",
    "    dimension_separator=\"/\",\n",
    ")\n",
    "\n",
    "instance_layer = viewer.add_labels(instance_layer, name=\"Instance\", scale=data_layer.scale)\n",
    "\n",
    "instance_painting_data = zarr.open(\n",
    "    f\"{zarr_path}/{instance_painting_name}\",\n",
    "    mode='a',\n",
    "    shape=image.shape[:2],\n",
    "    dtype='i4',\n",
    "    dimension_separator=\"/\",\n",
    ")\n",
    "\n",
    "instance_painting_layer = viewer.add_labels(instance_painting_data, name=\"Instance Painting\", scale=data_layer.scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a UI as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qtpy.QtWidgets import (\n",
    "    QVBoxLayout,\n",
    "    QHBoxLayout,\n",
    "    QComboBox,\n",
    "    QLabel,\n",
    "    QCheckBox,\n",
    "    QDoubleSpinBox,\n",
    "    QGroupBox,\n",
    "    QWidget,\n",
    ")\n",
    "\n",
    "class NapariMLWidget(QWidget):\n",
    "    def __init__(self, parent=None):\n",
    "        super(NapariMLWidget, self).__init__(parent)\n",
    "\n",
    "        self.initUI()\n",
    "\n",
    "    def initUI(self):\n",
    "        layout = QVBoxLayout()\n",
    "\n",
    "        # Dropdown for selecting the model\n",
    "        model_label = QLabel(\"Select Model\")\n",
    "        self.model_dropdown = QComboBox()\n",
    "        self.model_dropdown.addItems([\"Random Forest\"])\n",
    "        model_layout = QHBoxLayout()\n",
    "        model_layout.addWidget(model_label)\n",
    "        model_layout.addWidget(self.model_dropdown)\n",
    "        layout.addLayout(model_layout)\n",
    "\n",
    "        # Select the range of sigma sizes\n",
    "        self.sigma_start_spinbox = QDoubleSpinBox()\n",
    "        self.sigma_start_spinbox.setRange(0, 256)\n",
    "        self.sigma_start_spinbox.setValue(1)\n",
    "\n",
    "        self.sigma_end_spinbox = QDoubleSpinBox()\n",
    "        self.sigma_end_spinbox.setRange(0, 256)\n",
    "        self.sigma_end_spinbox.setValue(5)\n",
    "\n",
    "        sigma_layout = QHBoxLayout()\n",
    "        sigma_layout.addWidget(QLabel(\"Sigma Range: From\"))\n",
    "        sigma_layout.addWidget(self.sigma_start_spinbox)\n",
    "        sigma_layout.addWidget(QLabel(\"To\"))\n",
    "        sigma_layout.addWidget(self.sigma_end_spinbox)\n",
    "        layout.addLayout(sigma_layout)\n",
    "\n",
    "        # Boolean options for features\n",
    "        self.intensity_checkbox = QCheckBox(\"Intensity\")\n",
    "        self.intensity_checkbox.setChecked(True)\n",
    "        self.edges_checkbox = QCheckBox(\"Edges\")\n",
    "        self.texture_checkbox = QCheckBox(\"Texture\")\n",
    "        self.texture_checkbox.setChecked(True)\n",
    "\n",
    "        features_group = QGroupBox(\"Features\")\n",
    "        features_layout = QVBoxLayout()\n",
    "        features_layout.addWidget(self.intensity_checkbox)\n",
    "        features_layout.addWidget(self.edges_checkbox)\n",
    "        features_layout.addWidget(self.texture_checkbox)\n",
    "        features_group.setLayout(features_layout)\n",
    "        layout.addWidget(features_group)\n",
    "\n",
    "        # Dropdown for data selection\n",
    "        data_label = QLabel(\"Select Data for Model Fitting\")\n",
    "        self.data_dropdown = QComboBox()\n",
    "        self.data_dropdown.addItems(\n",
    "            [\"Current Displayed Region\", \"Whole Image\"]\n",
    "        )\n",
    "        self.data_dropdown.setCurrentText(\"Current Displayed Region\")\n",
    "        data_layout = QHBoxLayout()\n",
    "        data_layout.addWidget(data_label)\n",
    "        data_layout.addWidget(self.data_dropdown)\n",
    "        layout.addLayout(data_layout)\n",
    "\n",
    "        # Checkbox for live model fitting\n",
    "        self.live_fit_checkbox = QCheckBox(\"Live Model Fitting\")\n",
    "        self.live_fit_checkbox.setChecked(True)\n",
    "        layout.addWidget(self.live_fit_checkbox)\n",
    "\n",
    "        # Checkbox for live prediction\n",
    "        self.live_pred_checkbox = QCheckBox(\"Live Prediction\")\n",
    "        self.live_pred_checkbox.setChecked(True)\n",
    "        layout.addWidget(self.live_pred_checkbox)\n",
    "\n",
    "        # Checkbox for live instance segmentation\n",
    "        self.live_instance_checkbox = QCheckBox(\"Live Instance Segmentation\")\n",
    "        self.live_instance_checkbox.setChecked(True)\n",
    "        layout.addWidget(self.live_instance_checkbox)\n",
    "        \n",
    "        self.setLayout(layout)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add this widget to napari\n",
    "\n",
    "widget = NapariMLWidget()\n",
    "viewer.window.add_dock_widget(widget, name=\"halfway to I2K 2023 America\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have a widget, we have our painting and prediction layers, now what?\n",
    "\n",
    "We need to start connecting things together. How should we do that? napari has things called \"events\" that happen when things happen within napari. We want to respond to a few different event types:\n",
    "- changes in camera (e.g. camera position and rotation)\n",
    "- changes in \"dims\" (e.g. moving a dimension slider)\n",
    "- painting events (e.g. a user clicked, painted, and release their mouse)\n",
    "\n",
    "When one of these events happens, we want to: \n",
    "- update our machine learning model with the new painted data\n",
    "- update our prediction with the updated ML model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with our event listener\n",
    "\n",
    "# We use \"curry\" because this allows us to \"store\" our viewer and widget for later use\n",
    "@tz.curry\n",
    "def on_data_change(event, viewer=None, widget=None):\n",
    "    corner_pixels = data_layer.corner_pixels\n",
    "\n",
    "    # Ensure the painting layer visual is updated\n",
    "    painting_layer.refresh()\n",
    "\n",
    "    # Training the ML model and generating predictions can take time\n",
    "    #   we will use a \"thread\" to perform these calculations\n",
    "    #   otherwise napari will freeze until these\n",
    "    # calculations are done\n",
    "    thread = threading.Thread(\n",
    "        target=threaded_on_data_change,\n",
    "        args=(\n",
    "            event,\n",
    "            corner_pixels,\n",
    "            viewer.dims,\n",
    "            widget.model_dropdown.currentText(),\n",
    "            {\n",
    "                \"sigma_min\": widget.sigma_start_spinbox.value(),\n",
    "                \"sigma_max\": widget.sigma_end_spinbox.value(),\n",
    "                \"intensity\": widget.intensity_checkbox.isChecked(),\n",
    "                \"edges\": widget.edges_checkbox.isChecked(),\n",
    "                \"texture\": widget.texture_checkbox.isChecked(),\n",
    "            },\n",
    "            widget.live_fit_checkbox.isChecked(),\n",
    "            widget.live_pred_checkbox.isChecked(),\n",
    "            widget.live_instance_checkbox.isChecked(),\n",
    "            widget.data_dropdown.currentText(),\n",
    "        ),\n",
    "    )\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "\n",
    "    # Ensure the prediction layer visual is updated\n",
    "    prediction_layer.refresh()\n",
    "    instance_layer.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have to make the hard part of the listener\n",
    "\n",
    "model = None\n",
    "\n",
    "def threaded_on_data_change(\n",
    "    event,\n",
    "    corner_pixels,\n",
    "    dims,\n",
    "    model_type,\n",
    "    feature_params,\n",
    "    live_fit,\n",
    "    live_prediction,\n",
    "    live_instance, \n",
    "    data_choice,\n",
    "):\n",
    "    global model\n",
    "    LOGGER.info(f\"Labels data has changed! {event}\")\n",
    "\n",
    "    current_step = dims.current_step\n",
    "\n",
    "    LOGGER.info(\"make mask\")\n",
    "    # Find a mask of indices we will use for fetching our data\n",
    "    mask_idx = (slice(corner_pixels[0, 0], corner_pixels[1, 0]), slice(corner_pixels[0, 1], corner_pixels[1, 1]))\n",
    "    #if data_choice == \"Whole Image\":\n",
    "    #    mask_idx = tuple([slice(0, sz) for sz in data_layer.data.shape])\n",
    "\n",
    "    LOGGER.info(f\"mask idx {mask_idx}, image {data_layer.data.shape}\")\n",
    "    active_image = data_layer.data[mask_idx]\n",
    "    LOGGER.info(\n",
    "        f\"active image shape {active_image.shape} data choice {data_choice} painting_data {painting_data.shape} mask_idx {mask_idx}\"\n",
    "    )\n",
    "\n",
    "    active_labels = painting_data[mask_idx]\n",
    "\n",
    "    def compute_features(image, feature_params):\n",
    "        \"\"\"Compute features for each channel and concatenate them.\"\"\"\n",
    "        features = extract_features(\n",
    "            image, feature_params\n",
    "        )\n",
    "\n",
    "        return features\n",
    "\n",
    "    training_labels = None\n",
    "\n",
    "    if data_choice == \"Current Displayed Region\":\n",
    "        # Use only the currently displayed region.\n",
    "        training_features = compute_features(\n",
    "            active_image, feature_params\n",
    "        )\n",
    "        training_labels = np.squeeze(active_labels)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid data choice: {data_choice}\")\n",
    "\n",
    "    if (training_labels is None) or np.any(training_labels.shape == 0):\n",
    "        LOGGER.info(\"No training data yet. Skipping model update\")\n",
    "    elif live_fit:\n",
    "        # Retrain model\n",
    "        LOGGER.info(\n",
    "            f\"training model with labels {training_labels.shape} features {training_features.shape} unique labels {np.unique(training_labels[:])}\"\n",
    "        )\n",
    "        model = update_model(training_labels, training_features, model_type)\n",
    "\n",
    "    # Don't do live prediction on whole image, that happens earlier slicewise\n",
    "    if live_prediction:\n",
    "        # Update prediction_data\n",
    "        prediction_features = compute_features(\n",
    "            active_image, feature_params\n",
    "        )\n",
    "        # Add 1 becasue of the background label adjustment for the model\n",
    "        prediction = predict(model, prediction_features, model_type)\n",
    "        LOGGER.info(\n",
    "            f\"prediction {prediction.shape} prediction layer {prediction_layer.data.shape} prediction {np.transpose(prediction).shape} features {prediction_features.shape}\"\n",
    "        )\n",
    "\n",
    "        #if data_choice == \"Whole Image\":\n",
    "        prediction_layer.data[mask_idx] = np.transpose(prediction)\n",
    "\n",
    "    if live_instance:\n",
    "        temp = prediction_layer.data[mask_idx].copy()\n",
    "        mask = active_labels > 0\n",
    "        #print('active labels ',active_labels.shape, type(active_labels) )\n",
    "        #print('the mask type is: !!!!', mask.shape, type(mask))\n",
    "        temp = np.where(mask, active_labels, temp)\n",
    "        mask = instance_painting_layer.data[mask_idx] > 0\n",
    "        temp = np.where(mask, instance_painting_layer.data[mask_idx], temp) \n",
    "        temp = temp-1\n",
    "        labels = label(temp)\n",
    "        instance_layer.data[mask_idx] = labels\n",
    "        #else:\n",
    "        #    prediction_layer.data[mask_idx] = np.transpose(prediction)[\n",
    "        #        np.newaxis, :\n",
    "        #    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training function that respects widget's model choice\n",
    "def update_model(labels, features, model_type):\n",
    "    features = features[labels > 0, :]\n",
    "    # We shift labels - 1 because background is 0 and has special meaning, but models need to start at 0\n",
    "    labels = labels[labels > 0] - 1\n",
    "    \n",
    "    if model_type == \"Random Forest\":\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=50, n_jobs=-1, max_depth=10, max_samples=0.05\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"updating model with label shape  {labels.shape} feature shape {features.shape} unique labels {np.unique(labels)}\"\n",
    "    )\n",
    "    \n",
    "    clf.fit(features, labels)\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "def predict(model, features, model_type):\n",
    "    # We shift labels + 1 because background is 0 and has special meaning\n",
    "    prediction = future.predict_segmenter(features.reshape(-1, features.shape[-1]), model).reshape(features.shape[:-1]) + 1\n",
    "\n",
    "    return np.transpose(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now connect everything together\n",
    "for listener in [\n",
    "    viewer.camera.events,\n",
    "    viewer.dims.events,\n",
    "    painting_layer.events.paint,\n",
    "    instance_painting_layer.events.paint,\n",
    "]:\n",
    "    listener.connect(\n",
    "        debounced(\n",
    "            ensure_main_thread(\n",
    "                on_data_change(\n",
    "                    viewer=viewer,\n",
    "                    widget=widget,  # pass the widget instance for easy access to settings\n",
    "                )\n",
    "            ),\n",
    "            timeout=1000,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Holes Save ML pixel classificaton result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from skimage.measure import label\n",
    "from scipy.ndimage import find_objects\n",
    "\n",
    "result = instance_layer.data[:]\n",
    "\n",
    "labels = label(result)\n",
    "\n",
    "# Create a filled labels array\n",
    "filled_labels = np.zeros_like(labels)\n",
    "\n",
    "# Get slices for each label (bounding boxes)\n",
    "slices = find_objects(labels)\n",
    "\n",
    "# Iterate over each label's bounding box\n",
    "for label_value, label_slice in enumerate(slices, start=1):\n",
    "    if label_slice is None:  # Skip missing labels\n",
    "        continue\n",
    "\n",
    "    # Crop to the bounding box\n",
    "    cropped_labels = labels[label_slice]\n",
    "\n",
    "    # Create a binary mask for the current label\n",
    "    binary_mask = cropped_labels == label_value\n",
    "\n",
    "    # Fill holes in the binary mask\n",
    "    filled_mask = binary_fill_holes(binary_mask)\n",
    "\n",
    "    # Assign filled mask back to the output array within the cropped region\n",
    "    filled_labels[label_slice][filled_mask] = label_value\n",
    "\n",
    "# Add to viewer\n",
    "viewer.add_labels(filled_labels, name=\"Filled Labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imsave\n",
    "\n",
    "# print layers of viewer\n",
    "print(viewer.layers)\n",
    "\n",
    "# get prediction layer\n",
    "result = viewer.layers[1].data[:]\n",
    "result = viewer.layers['Instance'].data[:]  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(result)\n",
    "\n",
    "imsave(parent_path / (image_name+\"_segmented.tif\"), result)\n",
    "imsave(parent_path / (\"ml_hole_fill_segmented.png\"), filled_labels.astype('uint16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(result)\n",
    "\n",
    "imsave(parent_path / (image_name+\"_segmented.tif\"), result)\n",
    "imsave(parent_path / (\"ml_hole_fill_segmented.png\"), filled_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dresden-decon-test1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
