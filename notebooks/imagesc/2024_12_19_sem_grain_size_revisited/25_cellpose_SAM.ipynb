{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data\n",
    "\n",
    "This notebook uses the ```napari-easy-augment-batch-dl``` widget to explore and label the data.  If we have a model trained we can also predict using that model. \n",
    "\n",
    "Note:  ```napari-easy-augment-batch-dl``` is a useful tool, especially for labelling, but is currently under construction for other uses.  Right now it **may** be best to use it for labelling and inspecting predictions and do other steps of the deep learning workflow (making patches, training) in notebooks.  (of course you are welcome to try the GUI for other steps and report any hiccups (or disasters) that occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raster_geometry not imported.  This is only needed for the ellipsoid rendering in apply_stardist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bnorthan/mambaforge/envs/easy_augment_pytorch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.0 (you have 1.4.12). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "/home/bnorthan/mambaforge/envs/easy_augment_pytorch/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/bnorthan/mambaforge/envs/easy_augment_pytorch/lib/python3.12/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/bnorthan/code/i2k/tnia/segment-everything/src/segment_everything/vendored/efficientvit/models/nn/ops.py:407: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled=False)\n"
     ]
    }
   ],
   "source": [
    "import napari\n",
    "viewer = napari.Viewer()\n",
    "from cellpose import models, io\n",
    "import os\n",
    "import numpy as np\n",
    "from napari_easy_augment_batch_dl import easy_augment_batch_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found framework is  PytorchSemanticFramework\n",
      "found framework is  RandomForestFramework\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(parent_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m model_name \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mbatch_dl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_image_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# optionally set a pretrained model and settings so we can do prediction\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/i2k/tnia/napari-easy-augment-batch-dl/src/napari_easy_augment_batch_dl/easy_augment_batch_dl.py:290\u001b[0m, in \u001b[0;36mNapariEasyAugmentBatchDL.load_image_directory\u001b[0;34m(self, parent_path)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# else get num_classes\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     num_classes, ok \u001b[38;5;241m=\u001b[39m QInputDialog\u001b[38;5;241m.\u001b[39mgetInt(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Classes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the number of classes (less than 8):\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeep_learning_project \u001b[38;5;241m=\u001b[39m \u001b[43mDeepLearningProject\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeep_learning_widgets \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeep_learning_project\u001b[38;5;241m.\u001b[39mframeworks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/code/i2k/tnia/napari-easy-augment-batch-dl/src/napari_easy_augment_batch_dl/deep_learning_project.py:292\u001b[0m, in \u001b[0;36mDeepLearningProject.__init__\u001b[0;34m(self, parent_path, num_classes)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     max_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 292\u001b[0m ml_labels_store \u001b[38;5;241m=\u001b[39m \u001b[43mmanage_zarr_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mml_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml_labels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_file_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m ml_features_store \u001b[38;5;241m=\u001b[39m manage_zarr_store(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mml_path,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mml_features\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_file_list, (max_y, max_x, max_channels\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m12\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mml_labels \u001b[38;5;241m=\u001b[39m ml_labels_store[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/code/i2k/tnia/napari-easy-augment-batch-dl/src/napari_easy_augment_batch_dl/zarr_helper.py:81\u001b[0m, in \u001b[0;36mmanage_zarr_store\u001b[0;34m(zarr_path, file_names, image_shape, dtype)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Resize datasets to accommodate new files\u001b[39;00m\n\u001b[1;32m     80\u001b[0m new_num_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(existing_filenames) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(new_filenames)\n\u001b[0;32m---> 81\u001b[0m \u001b[43mz\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mresize(new_num_files, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     82\u001b[0m z[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilenames\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mresize(new_num_files)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Add new file names and placeholders for their results\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/easy_augment_pytorch/lib/python3.12/site-packages/zarr/hierarchy.py:511\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(item)\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(item)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'results'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_dl = easy_augment_batch_dl.NapariEasyAugmentBatchDL(viewer, label_only = False)\n",
    "\n",
    "viewer.window.add_dock_widget(\n",
    "    batch_dl\n",
    ")\n",
    "\n",
    "\n",
    "parent_path =r'D:\\images\\tnia-python-images\\\\imagesc\\\\2024_12_19_sem_grain_size_revisit'\n",
    "parent_path = r'/home/bnorthan/images/tnia-python-images/imagesc/2024_12_19_sem_grain_size_revisit'\n",
    "model_path = os.path.join(parent_path, 'models')\n",
    "model_name =  None\n",
    "batch_dl.load_image_directory(parent_path)\n",
    "\n",
    "# optionally set a pretrained model and settings so we can do prediction\n",
    "if model_name is not None:\n",
    "    batch_dl.network_architecture_drop_down.setCurrentText(model_type)\n",
    "    batch_dl.deep_learning_project.set_pretrained_model(os.path.join(model_path, model_name), model_type)\n",
    "\n",
    "    model = batch_dl.deep_learning_project.models[model_type]\n",
    "    model.prob_thresh = -1\n",
    "    model.flow_thresh = 0.4\n",
    "    model.chan_segment = 2\n",
    "    model.chan2 = 3\n",
    "\n",
    "    widget = batch_dl.param_widgets[model_type]\n",
    "    widget.sync_with_model()\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 65535)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_path =r'D:\\images\\tnia-python-images\\\\imagesc\\\\2024_10_07_cellpose_multi_nuclear'\n",
    "\n",
    "test = io.imread(os.path.join(parent_path, 'Empty_02 - Copy.tif')).astype('uint16')\n",
    "\n",
    "test.min(), test.max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_and_SAM3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
