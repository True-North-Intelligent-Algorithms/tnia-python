{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional installs\n",
    "\n",
    "```\n",
    "pip rasterio\n",
    "pip colorcet\n",
    "pip aicsimageio\n",
    "pip install --upgrade urlib3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model brightfield_nuclei version 0.1.0 downloaded and extracted to c:\\Users\\bnort\\miniconda3\\envs\\microsam_cellose_sam\\Lib\\site-packages\\instanseg\\utils\\../bioimageio_models/\n",
      "Requesting default device: cuda\n"
     ]
    }
   ],
   "source": [
    "from instanseg import InstanSeg\n",
    "instanseg_brightfield = InstanSeg(\"brightfield_nuclei\", image_reader= \"tiffslide\", verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting default device: cuda\n",
      "### Device set: cuda ###\n"
     ]
    }
   ],
   "source": [
    "model = InstanSegModel(\"fluorescence_nuclei_and_cells\", gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<tifffile.TiffFile 'TestHidden_002.tif'> OME series cannot handle discontiguous storage ((1920, 2560, 3) != (3, 1920, 2560))\n"
     ]
    }
   ],
   "source": [
    "from skimage.io import imread\n",
    "import os\n",
    "parent_path =r'D:\\images\\tnia-python-images\\imagesc\\2024_03_27_SOTA_segmentation\\images'\n",
    "im_name = 'cell_00009.tif'\n",
    "\n",
    "parent_path = r'D:\\images\\tnia-python-images\\imagesc\\2025_05_10_SOTA_Test_Set'\n",
    "label_path = r'D:\\images\\tnia-python-images\\imagesc\\2025_05_10_SOTA_Test_Set\\annotations\\class_0'\n",
    "im_name = 'TestHidden_002.tif'\n",
    "\n",
    "im = imread(os.path.join(parent_path, im_name))\n",
    "label = imread(os.path.join(label_path, im_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on InstanSeg in module instanseg.inference_class object:\n",
      "\n",
      "class InstanSeg(builtins.object)\n",
      " |  InstanSeg(model_type: Union[str, torch.nn.modules.module.Module] = 'brightfield_nuclei', device: Optional[str] = None, image_reader: str = 'tiffslide', verbosity: int = 1)\n",
      " |  \n",
      " |  Main class for running InstanSeg.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model_type: Union[str, torch.nn.modules.module.Module] = 'brightfield_nuclei', device: Optional[str] = None, image_reader: str = 'tiffslide', verbosity: int = 1)\n",
      " |      :param model_type: The type of model to use. If a string is provided, the model will be downloaded. If the model is not public, it will look for a model in your bioimageio folder. If an nn.Module is provided, this model will be used.\n",
      " |      :param device: The device to run the model on. If None, the device will be chosen automatically.\n",
      " |      :param image_reader: The image reader to use. Options are \"tiffslide\", \"skimage.io\", \"bioio\", \"AICSImageIO\".\n",
      " |      :param verbosity: The verbosity level. 0 is silent, 1 is normal, 2 is verbose.\n",
      " |  \n",
      " |  display(self, image: <built-in method tensor of type object at 0x00007FFF202BF550>, instances: torch.Tensor, normalise: bool = True) -> numpy.ndarray\n",
      " |      Save the output of an InstanSeg model overlaid on the input.\n",
      " |      See :func:`save_image_with_label_overlay <instanseg.utils.save_image_with_label_overlay>` for more details and return types.\n",
      " |      :param image: The input image.\n",
      " |      :param instances: The output labels.\n",
      " |  \n",
      " |  eval(self, image: Union[str, List[str]], pixel_size: Optional[float] = None, save_output: bool = False, save_overlay: bool = False, save_geojson: bool = False, processing_method: str = 'auto', **kwargs) -> Union[torch.Tensor, List[torch.Tensor], NoneType]\n",
      " |      Evaluate the input image or list of images using the InstanSeg model.\n",
      " |      :param image: The path to the image, or a list of such paths.\n",
      " |      :param pixel_size: The pixel size in microns.\n",
      " |      :param save_output: Controls whether the output is saved to disk (see :func:`save_output <instanseg.Instanseg.save_output>`).\n",
      " |      :param save_overlay: Controls whether the output is saved to disk as an overlay (see :func:`save_output <instanseg.Instanseg.save_output>`).\n",
      " |      :param save_geojson: Controls whether the geojson output labels are saved to disk (see :func:`save_output <instanseg.Instanseg.save_output>`).\n",
      " |      :param processing_method: The processing method to use. Options are \"auto\", \"small\", \"medium\", \"wsi\". If \"auto\", the method will be chosen based on the size of the image.\n",
      " |      :param kwargs: Passed to other eval methods, eg :func:`save_output <instanseg.Instanseg.eval_small_image>`, :func:`save_output <instanseg.Instanseg.eval_medium_image>`, :func:`save_output <instanseg.Instanseg.eval_whole_slide_image>` \n",
      " |      :return: A torch.Tensor of outputs if the input is a path to a single image, or a list of such outputs if the input is a list of paths, or None if the input is a whole slide image.\n",
      " |  \n",
      " |  eval_medium_image(self, image: torch.Tensor, pixel_size: Optional[float] = None, normalise: bool = True, tile_size: int = 512, batch_size: int = 1, return_image_tensor: bool = True, normalisation_subsampling_factor: int = 1, target: str = 'all_outputs', rescale_output: bool = True, **kwargs) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\n",
      " |      Evaluate a medium input image using the InstanSeg model. The image will be split into tiles, and then inference and object merging will be handled internally.\n",
      " |      \n",
      " |      :param image:: The input image(s) to be evaluated.\n",
      " |      :param pixel_size: The pixel size of the image, in microns. If not provided, it will be read from the image metadata.\n",
      " |      :param normalise: Controls whether the image is normalised.\n",
      " |      :param tile_size: The width/height of the tiles that the image will be split into.\n",
      " |      :param batch_size: The number of tiles to be run simultaneously.\n",
      " |      :param return_image_tensor: Controls whether the input image is returned as part of the output.\n",
      " |      :param normalisation_subsampling_factor: The subsampling or downsample factor at which to calculate normalisation parameters.\n",
      " |      :param target: Controls what type of output is given, usually \"all_outputs\", \"nuclei\", or \"cells\".\n",
      " |      :param rescale_output: Controls whether the outputs should be rescaled to the same coordinate space as the input (useful if the pixel size is different to that of the InstanSeg model being used).\n",
      " |      :param kwargs: Passed to pytorch.\n",
      " |      \n",
      " |      :return: A tensor corresponding to the output targets specified, as well as the input image if requested.\n",
      " |  \n",
      " |  eval_small_image(self, image: torch.Tensor, pixel_size: Optional[float] = None, normalise: bool = True, return_image_tensor: bool = True, target: str = 'all_outputs', rescale_output: bool = True, **kwargs) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\n",
      " |      Evaluate a small input image using the InstanSeg model.\n",
      " |      \n",
      " |      :param image:: The input image(s) to be evaluated.\n",
      " |      :param pixel_size: The pixel size of the image, in microns. If not provided, it will be read from the image metadata.\n",
      " |      :param normalise: Controls whether the image is normalised.\n",
      " |      :param return_image_tensor: Controls whether the input image is returned as part of the output.\n",
      " |      :param target: Controls what type of output is given, usually \"all_outputs\", \"nuclei\", or \"cells\".\n",
      " |      :param rescale_output: Controls whether the outputs should be rescaled to the same coordinate space as the input (useful if the pixel size is different to that of the InstanSeg model being used).\n",
      " |      :param kwargs: Passed to pytorch.\n",
      " |      \n",
      " |      :return: A tensor corresponding to the output targets specified, as well as the input image if requested.\n",
      " |  \n",
      " |  eval_whole_slide_image(self, image: str, pixel_size: Optional[float] = None, normalise: bool = True, normalisation_subsampling_factor: int = 1, tile_size: int = 512, overlap: int = 80, detection_size: int = 20, save_geojson: bool = False, use_otsu_threshold: bool = False, **kwargs)\n",
      " |      Evaluate a whole slide input image using the InstanSeg model. This function uses slideio to read an image and then segments it using the instanseg model. The segmentation is done in a tiled manner to avoid memory issues. \n",
      " |      \n",
      " |      :param image: The input image to be evaluated.\n",
      " |      :param pixel_size: The pixel size of the image, in microns. If not provided, it will be read from the image metadata.\n",
      " |      :param normalise: Controls whether the image is normalised.\n",
      " |      :param tile_size: The width/height of the tiles that the image will be split into.\n",
      " |      :param overlap: The overlap (in pixels) betwene tiles.\n",
      " |      :param detection_size: The expected maximum size of detection objects.\n",
      " |      :param batch_size: The number of tiles to be run simultaneously.\n",
      " |      :param normalisation_subsampling_factor: The subsampling or downsample factor at which to calculate normalisation parameters.\n",
      " |      :param use_otsu_threshold: bool = False. Whether to use an otsu threshold on the image thumbnail to find the tissue region.\n",
      " |      :param kwargs: Passed to pytorch.\n",
      " |      :return: Returns a zarr file with the segmentation. The zarr file is saved in the same directory as the image with the same name but with the extension .zarr.\n",
      " |  \n",
      " |  read_image(self, image_str: str, processing_method='auto') -> Union[Tuple[str, float], Tuple[numpy.ndarray, float]]\n",
      " |      Read an image file from disk.\n",
      " |      :param image_str: The path to the image.\n",
      " |      :param processing_method: The processing method to use. Options are \"auto\", \"small\", \"medium\", \"wsi\". If \"auto\", the method will be chosen based on the size of the image.\n",
      " |      :return: The image array if it can be safely read (or the path to the image if it cannot) and the pixel size in microns.\n",
      " |  \n",
      " |  read_pixel_size(self, image_str: str) -> float\n",
      " |      Read the pixel size from an image on disk.\n",
      " |      :param image_str: The path to the image.\n",
      " |      :return: The pixel size in microns.\n",
      " |  \n",
      " |  read_slide(self, image_str: str)\n",
      " |      Read a whole slide image from disk.\n",
      " |      :param image_str: The path to the image.\n",
      " |  \n",
      " |  save_output(self, image_path: str, labels: torch.Tensor, image_array: Optional[numpy.ndarray] = None, save_output: bool = True, save_overlay=False, save_geojson=False) -> None\n",
      " |      Save the output of InstanSeg to disk.\n",
      " |      :param image_path: The path to the image, and where outputs will be saved.\n",
      " |      :param labels: The output labels.\n",
      " |      :param image_array: The image in array format. Required to save overlay.\n",
      " |      :param save_output: Save the labels to disk.\n",
      " |      :param save_overlay: Save the labels overlaid on the image.\n",
      " |      :param save_geojson: Save the labels as a GeoJSON feature collection.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(instanseg_brightfield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<tifffile.TiffFile 'TestHidden_002.tif'> OME series cannot handle discontiguous storage ((1920, 2560, 3) != (3, 1920, 2560))\n",
      "<tifffile.TiffFile 'TestHidden_002.tif'> OME series cannot handle discontiguous storage ((1920, 2560, 3) != (3, 1920, 2560))\n",
      "<tifffile.TiffFile 'TestHidden_002.tif'> OME series cannot handle discontiguous storage ((1920, 2560, 3) != (3, 1920, 2560))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conflicting sizes for dimension 'C': length 3 on the data but length 1 on coordinate 'C'\n",
      "Could not read pixel size from image metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             1.03it/s]\r"
     ]
    }
   ],
   "source": [
    "test = instanseg_brightfield.eval(parent_path +'\\\\' + im_name, pixel_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'Labels' at 0x218c4ed6810>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import napari\n",
    "import numpy as np\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(im)\n",
    "viewer.add_labels(label)  # Add the label image as labels\n",
    "viewer.add_labels(np.squeeze(test[0].cpu().numpy().astype('uint16')))  # Add the first output as labels\n",
    "#viewer.add_labels(np.squeeze(test[1].cpu().numpy().astype('uint16')))  # Add the first output as labels\n",
    "#viewer.add_labels(int(np.squeeze(test[1].cpu().numpy())))  # Add the second output as labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m.shape\n",
      "\u001b[31mIndexError\u001b[39m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "test[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microsam_cellose_sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
