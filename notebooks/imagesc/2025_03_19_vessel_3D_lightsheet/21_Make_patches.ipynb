{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make patches from labels\n",
    "\n",
    "Make a set of augmented patches from labels.\n",
    "\n",
    "As a reminder, in this project\n",
    "\n",
    "1.  Annotations are the set of 'marked up' image with pixels of objects assigned unique labels.  It may not be desired to use all annotations for training. \n",
    "1.  Labels are a set of annotations (marked by bounding boxes) we want to use for training.  \n",
    "2.  Patches are a set of images and annotated ground truths generated from labels.  They are usually cropped from labels to be all the same size.  They are often augmented.  1 label can be used to generate many patches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first cell we set up paths to the label images, and get a list of the file names of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from tnia.plotting.plt_helper import imshow_multi2d\n",
    "import numpy as np\n",
    "from tnia.deeplearning.dl_helper import quantile_normalization\n",
    "import json\n",
    "from tnia.deeplearning.dl_helper import get_label_paths\n",
    "from tnia.plotting.plt_helper import random_label_cmap\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "parent_path = r'D:\\images\\tnia-python-images\\imagesc\\2025_03_19_vessel_3D_lightsheet'\n",
    "parent_path = Path(parent_path)\n",
    "label_path = Path(parent_path / r'labels')\n",
    "\n",
    "# open the info file\n",
    "json_ = json.load(open(label_path / 'info.json'))\n",
    "\n",
    "# get number of inputs and number of ground truths for this problem\n",
    "num_inputs = json_['num_inputs']\n",
    "num_ground_truths = json_['num_truths']\n",
    "print('num inputs: ', num_inputs)   \n",
    "print('num ground truth: ', num_ground_truths)\n",
    "\n",
    "image_label_paths, ground_truths_label_paths = get_label_paths(1, num_ground_truths, label_path)\n",
    "print('image label paths',image_label_paths)\n",
    "print(\"ground_truth_label_paths\", ground_truths_label_paths[0])\n",
    "\n",
    "# get list of tif files in image_label_path\n",
    "tif_files = glob(str(os.path.join(image_label_paths[0], '*.tif')))\n",
    "print()\n",
    "print('The following tif files were found in the image label path: ')\n",
    "for tif_file in tif_files:\n",
    "    print(\"tif_file: \", tif_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display one of the label/ground truth sets\n",
    "\n",
    "This is done just to check the data looks OK.  Change ```i``` to look at different image/label pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "\n",
    "# get tif name\n",
    "tif_name = os.path.basename(tif_files[i])\n",
    "print('tiff name is', tif_name)\n",
    "\n",
    "image = imread(str(image_label_paths[0]/ (tif_name)))\n",
    "\n",
    "# read labels (there can be more than one class)\n",
    "labels = []\n",
    "for ground_truths_label_path in ground_truths_label_paths:\n",
    "    label = imread(os.path.join(ground_truths_label_path / (tif_name)))\n",
    "    labels.append(label)\n",
    "\n",
    "# show labels image \n",
    "images_to_show=[]\n",
    "titles = []\n",
    "\n",
    "if image.max() > 255:\n",
    "    image = image/image.max()\n",
    "\n",
    "print('image shape', image.shape)\n",
    "print('image dtype', image.dtype)\n",
    "print('image max', image.max())\n",
    "print('label max', label.max())\n",
    "\n",
    "images_to_show.append(image)\n",
    "titles.append(\"image\")\n",
    "\n",
    "for label in labels:\n",
    "    images_to_show.append(label)\n",
    "    titles.append(\"mask\")\n",
    "\n",
    "fig = imshow_multi2d(images_to_show, titles, 1, len(images_to_show), width=20, height=10, colormaps = ['gray', random_label_cmap()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment and normalize images\n",
    "\n",
    "Note: Normalization is always tricky.  We want to normalize based on the expected range in real images, which are often larger than training patches.  So here we normalize first, then extract the patches, so that all patches are normalized based on the expected intensity range of full sized images (not intensity range of the patch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tnia.deeplearning.augmentation import uber_augmenter\n",
    "i=0\n",
    "\n",
    "num_patches = 200\n",
    "\n",
    "for i in range(len(tif_files)):\n",
    "    # get tif name\n",
    "    tif_name = os.path.basename(tif_files[i])\n",
    "    print('tiff name is', tif_name)\n",
    "    image = imread(str(image_label_paths[0]/ (tif_name)))\n",
    "\n",
    "    # NOTE:  We need to get the same type of normalization as the training data\n",
    "    image = quantile_normalization(image, channels=True).astype(np.float32)\n",
    "    print(image.shape)\n",
    "    \n",
    "    labels = []\n",
    "    for ground_truths_label_path in ground_truths_label_paths:\n",
    "        label_ = imread(os.path.join(ground_truths_label_path / (tif_name)))\n",
    "        labels.append(label_)\n",
    "\n",
    "    patch_path= parent_path / 'patches' \n",
    "\n",
    "    if not os.path.exists(patch_path):\n",
    "        os.mkdir(patch_path)\n",
    "    axes = 'YXC'\n",
    "    sub_sample = 1\n",
    "\n",
    "    uber_augmenter(image, labels, patch_path, 'grid', 512, num_patches, do_random_gamma=True, do_elastic_transform=False, sub_sample_xy=1, size_factor = 10, sigma=50, alpha_affine=50, alpha=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_and_SAM3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
