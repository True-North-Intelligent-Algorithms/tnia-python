{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from cellpose import denoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    \"\"\"Load all images from a folder as NumPy arrays and store their filenames.\"\"\"\n",
    "    images = []\n",
    "    filenames = []\n",
    "    for filename in sorted(os.listdir(folder)):  # Sort to maintain order\n",
    "        if filename.endswith(('.tif', '.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            img = Image.open(img_path).convert('RGB')  # Convert to RGB\n",
    "            img_array = np.array(img)  # Convert image to NumPy array\n",
    "            images.append(img_array)\n",
    "            filenames.append(filename)  # Store original filename\n",
    "    return images, filenames  # Return images as a list, not a single NumPy array\n",
    "\n",
    "def process_cellpose_output(img_array):\n",
    "    \"\"\"Convert Cellpose output to a format compatible with PIL while preserving channels.\"\"\"\n",
    "    img_array = np.squeeze(img_array)  # Remove singleton dimensions\n",
    "\n",
    "    # Normalize if float32 (scale to 0-255)\n",
    "    if img_array.dtype == np.float32:\n",
    "        img_array = (img_array - img_array.min()) / (img_array.max() - img_array.min()) * 255\n",
    "        img_array = img_array.astype(np.uint8)  # Convert to 8-bit\n",
    "    \n",
    "    # If the image is multi-channel (e.g., RGB), ensure all channels are handled correctly\n",
    "    if img_array.ndim == 3:  # Multi-channel (RGB or other)\n",
    "        img_array = np.clip(img_array, 0, 255)  # Ensure values are within byte range (0-255)\n",
    "\n",
    "    return img_array\n",
    "\n",
    "def save_restored_images(filenames, restored_images, output_folder):\n",
    "    \"\"\"Save restored images as .tif files with original names.\"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Ensure output folder exists\n",
    "\n",
    "    for filename, img_array in zip(filenames, restored_images):\n",
    "        save_path = os.path.join(output_folder, filename)  # Keep original name\n",
    "        img = Image.fromarray(img_array.astype(np.uint8))  # Convert to 8-bit TIFF\n",
    "        img.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1462, 1702, 2)\n",
      "(1702, 2)\n",
      "(1794, 1737, 2)\n",
      "(1737, 2)\n",
      "(1785, 1676, 2)\n",
      "(1676, 2)\n",
      "(1706, 1789, 2)\n",
      "(1789, 2)\n",
      "(1072, 1433, 2)\n",
      "(1433, 2)\n",
      "(1207, 1565, 2)\n",
      "(1565, 2)\n",
      "(1786, 1680, 2)\n",
      "(1680, 2)\n",
      "(1597, 1757, 2)\n",
      "(1757, 2)\n",
      "(1782, 1706, 2)\n",
      "(1706, 2)\n",
      "(1381, 1445, 2)\n",
      "(1445, 2)\n",
      "(1492, 1713, 2)\n",
      "(1713, 2)\n"
     ]
    }
   ],
   "source": [
    "# === RUN PIPELINE ===\n",
    "input_folder = r\"D:\\images\\tnia-python-images\\imagesc\\2025_02_13_cellpose_restoration\"\n",
    "output_folder = r\"D:\\images\\tnia-python-images\\imagesc\\2025_02_13_cellpose_restoration\\restored\"\n",
    "\n",
    "# Step 1: Load Images\n",
    "imgs, filenames = load_images_from_folder(input_folder)\n",
    "\n",
    "# Step 2: Restore Images using Cellpose (Process Each Image Individually)\n",
    "dn = denoise.DenoiseModel(model_type=\"denoise_cyto2\", gpu=True)\n",
    "\n",
    "restored_imgs = []\n",
    "for img in imgs:\n",
    "    img_dn = dn.eval(np.expand_dims(img, axis=0), channels=[1,2,3], diameter=15.)  # Process one image at a time\n",
    "    #img_dn = dn.eval(np.expand_dims(img, axis=0), channels=[1,2,3], diameter=15.)  # Process one image at a time\n",
    "    print(img_dn.shape)\n",
    "    print(img_dn[0].shape)\n",
    "    restored_img = process_cellpose_output(img_dn)  # Get the first (and only) image in the batch\n",
    "    restored_imgs.append(restored_img)  # Store the restored image\n",
    "\n",
    "# Step 3: Save Restored Images with Original Names\n",
    "save_restored_images(filenames, restored_imgs, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DenoiseModel in module cellpose.denoise:\n",
      "\n",
      "class DenoiseModel(builtins.object)\n",
      " |  DenoiseModel(gpu=False, pretrained_model=False, nchan=1, model_type=None, chan2=False, diam_mean=30.0, device=None)\n",
      " |  \n",
      " |  DenoiseModel class for denoising images using Cellpose denoising model.\n",
      " |  \n",
      " |  Args:\n",
      " |      gpu (bool, optional): Whether to use GPU for computation. Defaults to False.\n",
      " |      pretrained_model (bool or str or Path, optional): Pretrained model to use for denoising.\n",
      " |          Can be a string or path. Defaults to False.\n",
      " |      nchan (int, optional): Number of channels in the input images, all Cellpose 3 models were trained with nchan=1. Defaults to 1.\n",
      " |      model_type (str, optional): Type of pretrained model to use (\"denoise_cyto3\", \"deblur_cyto3\", \"upsample_cyto3\", ...). Defaults to None.\n",
      " |      chan2 (bool, optional): Whether to use a separate model for the second channel. Defaults to False.\n",
      " |      diam_mean (float, optional): Mean diameter of the objects in the images. Defaults to 30.0.\n",
      " |      device (torch.device, optional): Device to use for computation. Defaults to None.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      nchan (int): Number of channels in the input images.\n",
      " |      diam_mean (float): Mean diameter of the objects in the images.\n",
      " |      net (CPnet): Cellpose network for denoising.\n",
      " |      pretrained_model (bool or str or Path): Pretrained model path to use for denoising.\n",
      " |      net_chan2 (CPnet or None): Cellpose network for the second channel, if applicable.\n",
      " |      net_type (str): Type of the denoising network.\n",
      " |  \n",
      " |  Methods:\n",
      " |      eval(x, batch_size=8, channels=None, channel_axis=None, z_axis=None,\n",
      " |              normalize=True, rescale=None, diameter=None, tile=True, tile_overlap=0.1)\n",
      " |          Denoise array or list of images using the denoising model.\n",
      " |  \n",
      " |      _eval(net, x, normalize=True, rescale=None, diameter=None, tile=True,\n",
      " |              tile_overlap=0.1)\n",
      " |          Run denoising model on a single channel.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, gpu=False, pretrained_model=False, nchan=1, model_type=None, chan2=False, diam_mean=30.0, device=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  eval(self, x, batch_size=8, channels=None, channel_axis=None, z_axis=None, normalize=True, rescale=None, diameter=None, tile=True, do_3D=False, tile_overlap=0.1, bsize=224)\n",
      " |      Restore array or list of images using the image restoration model.\n",
      " |      \n",
      " |      Args:\n",
      " |          x (list, np.ndarry): can be list of 2D/3D/4D images, or array of 2D/3D/4D images\n",
      " |          batch_size (int, optional): number of 224x224 patches to run simultaneously on the GPU\n",
      " |              (can make smaller or bigger depending on GPU memory usage). Defaults to 8.\n",
      " |          channels (list, optional): list of channels, either of length 2 or of length number of images by 2.\n",
      " |              First element of list is the channel to segment (0=grayscale, 1=red, 2=green, 3=blue).\n",
      " |              Second element of list is the optional nuclear channel (0=none, 1=red, 2=green, 3=blue).\n",
      " |              For instance, to segment grayscale images, input [0,0]. To segment images with cells\n",
      " |              in green and nuclei in blue, input [2,3]. To segment one grayscale image and one\n",
      " |              image with cells in green and nuclei in blue, input [[0,0], [2,3]].\n",
      " |              Defaults to None.\n",
      " |          channel_axis (int, optional): channel axis in element of list x, or of np.ndarray x. \n",
      " |              if None, channels dimension is attempted to be automatically determined. Defaults to None.\n",
      " |          z_axis  (int, optional): z axis in element of list x, or of np.ndarray x. \n",
      " |              if None, z dimension is attempted to be automatically determined. Defaults to None.\n",
      " |          normalize (bool, optional): if True, normalize data so 0.0=1st percentile and 1.0=99th percentile of image intensities in each channel; \n",
      " |              can also pass dictionary of parameters (all keys are optional, default values shown): \n",
      " |                  - \"lowhigh\"=None : pass in normalization values for 0.0 and 1.0 as list [low, high] (if not None, all following parameters ignored)\n",
      " |                  - \"sharpen\"=0 ; sharpen image with high pass filter, recommended to be 1/4-1/8 diameter of cells in pixels\n",
      " |                  - \"normalize\"=True ; run normalization (if False, all following parameters ignored)\n",
      " |                  - \"percentile\"=None : pass in percentiles to use as list [perc_low, perc_high]\n",
      " |                  - \"tile_norm\"=0 ; compute normalization in tiles across image to brighten dark areas, to turn on set to window size in pixels (e.g. 100)\n",
      " |                  - \"norm3D\"=False ; compute normalization across entire z-stack rather than plane-by-plane in stitching mode.\n",
      " |              Defaults to True.\n",
      " |          rescale (float, optional): resize factor for each image, if None, set to 1.0;\n",
      " |              (only used if diameter is None). Defaults to None.\n",
      " |          diameter (float, optional):  diameter for each image, \n",
      " |              if diameter is None, set to diam_mean or diam_train if available. Defaults to None.\n",
      " |          tile_overlap (float, optional): fraction of overlap of tiles when computing flows. Defaults to 0.1.\n",
      " |          \n",
      " |      Returns:\n",
      " |          imgs (list of 2D/3D arrays): Restored images\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(denoise.DenoiseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_restored_images(filenames, restored_imgs, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_and_SAM3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
