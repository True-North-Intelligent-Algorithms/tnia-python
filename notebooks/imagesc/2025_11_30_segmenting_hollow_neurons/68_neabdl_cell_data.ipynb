{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy-Augment-Batch-DL with Cell Data\n",
    "\n",
    "This notebook demonstrates how to use the napari-easy-augment-batch-dl plugin to train deep learning models on cell segmentation data.\n",
    "\n",
    "### Background\n",
    "\n",
    "The easy-augment-batch-dl plugin provides a streamlined workflow for training deep learning models with minimal data. It has panes that allow you to\n",
    "\n",
    "1.  Draw labels\n",
    "2.  Augment labels  \n",
    "3.  Train/predict with different architectures\n",
    "\n",
    "### Semantic vs Instance Segmentation\n",
    "\n",
    "**Semantic Segmentation**: Every pixel gets classified into a category (background, vessel, cell, etc.)\n",
    "**Instance Segmentation**: Individual objects are separated (cell 1, cell 2, cell 3, etc.)\n",
    "\n",
    "For this cell example, we'll use instance segmentation to identify individual cells.\n",
    "\n",
    "### Labeling Strategy\n",
    "\n",
    "**Sparse Labeling**:  Not every pixel has to be labeled.  However some background needs to be labeled to differentiate between background pixels and unlabeled pixels.  For example if there were 2 foreground classes use label 1 for background, 2 for \"class 1\", and 3 for \"class 2\".  Unlabeled pixels (0) will be ignored. \n",
    "\n",
    "**Dense Labeling**:  Every foreground pixel needs to be labeled however background does not need to be labeled.  If there were 2 foreground classes, use 1 for \"class 1\", 2 for \"class 2\" and then the remaining pixels (0) will be treated as background. \n",
    "\n",
    "In this example we use dense labeling, as we label every object in the ROI. \n",
    "\n",
    "### Augmentation\n",
    "\n",
    "The plugin includes a panel with various augmentation options. Simply check the desired augmentation types (Horizontal Flip, Random Resize, Random Adjust Color, Elastic Deformation, etc.) and click \"**Augment All Images**\" or \"**Augment Single**\" to automatically generate additional training data from your labeled images.\n",
    "\n",
    "### Train and Predict\n",
    "\n",
    "After generating augmented patches, you can train a model. Use the dropdown menu to select your desired architecture (in the screenshot, `Micro-sam Instance Framework` is selected), then click \"**Train Network**\". Once training is complete, you can use your trained model to predict on new images. You also have the option to load a previously trained model if available.\n",
    "\n",
    "The below screenshot shows Napari-Easy-Augment-Batch open with Instance labels for individual cells drawn. \n",
    "\n",
    "![Easy Augment Label Instance](screenshots/003_easy_augment_labelling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data\n",
    "\n",
    "This notebook uses the ```napari-easy-augment-batch-dl``` widget to explore and label the data.  If we have a model trained we can also predict using that model. \n",
    "\n",
    "Note:  ```napari-easy-augment-batch-dl``` is a useful tool, especially for labelling, but is currently under construction for other uses.  Right now it **may** be best to use it for labelling and inspecting predictions and do other steps of the deep learning workflow (making patches, training) in notebooks.  (of course you are welcome to try the GUI for other steps and report and hiccups (or disasters) that occur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bnort\\work\\ImageJ2022\\tnia\\notebooks-and-napari-widgets-for-dl\\pixi\\microsam_cellposesam\\.pixi\\envs\\default\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'segment_everything'\n"
     ]
    }
   ],
   "source": [
    "from cellpose import models, io\n",
    "import os\n",
    "import numpy as np\n",
    "import napari\n",
    "from napari_easy_augment_batch_dl import easy_augment_batch_dl\n",
    "from napari_easy_augment_batch_dl.frameworks.micro_sam_instance_framework import MicroSamInstanceFramework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: QWindowsWindow::setGeometry: Unable to set geometry 1086x1108+2560+280 (frame: 1102x1147+2552+249) on QWidgetWindow/\"_QtMainWindowClassWindow\" on \"\\\\.\\DISPLAY2\". Resulting geometry: 1086x1061+2560+280 (frame: 1102x1100+2552+249) margins: 8, 31, 8, 8 minimum size: 385x497 MINMAXINFO maxSize=0,0 maxpos=0,0 mintrack=401,536 maxtrack=0,0)\n",
      "C:\\Users\\bnort\\AppData\\Local\\Temp\\ipykernel_2676\\1872267416.py:2: DeprecationWarning: The 'label_only' parameter is deprecated. Please use the 'mode' parameter instead.\n",
      "  batch_dl = easy_augment_batch_dl.NapariEasyAugmentBatchDL(viewer, label_only = False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'segment_everything'\n",
      "Found framework MicroSamInstanceFramework\n",
      "Found framework CellPoseInstanceFramework\n",
      "2025-11-30 07:51:25,064 [INFO] WRITING LOG OUTPUT TO C:\\Users\\bnort\\.cellpose\\run.log\n",
      "2025-11-30 07:51:25,066 [INFO] \n",
      "cellpose version: \t4.0.7 \n",
      "platform:       \twin32 \n",
      "python version: \t3.11.14 \n",
      "torch version:  \t2.6.0\n",
      "2025-11-30 07:51:25,067 [WARNING] model_type argument is not used in v4.0.1+. Ignoring this argument...\n",
      "2025-11-30 07:51:25,296 [INFO] ** TORCH CUDA version installed and working. **\n",
      "2025-11-30 07:51:25,298 [INFO] >>>> using GPU (CUDA)\n",
      "2025-11-30 07:51:27,239 [INFO] >>>> loading model C:\\Users\\bnort\\.cellpose\\models\\cpsam\n",
      "Found framework RandomForestFramework\n",
      "Found framework VesselsSemanticFramework\n",
      "Error creating ml labels and features: 'filenames'\n",
      "Error creating ml_labels: 'DeepLearningProject' object has no attribute 'ml_labels'\n",
      "Random Forest ML may not work properly\n",
      "Adding object boxes layer\n",
      "Adding predicted object boxes layer\n",
      "Adding label boxes\n",
      "Data changed\n",
      "Data changed\n",
      "Adding object boxes\n",
      "Adding predicted object boxes\n",
      "Setting object box classes\n",
      "Setting predicted object box classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\images\\tnia-python-images\\imagesc\\2025_11_30_segmenting_hollow_neurons\\patches\\tmpbkefvpq0 D:\\images\\tnia-python-images\\imagesc\\2025_11_30_segmenting_hollow_neurons\\patches\\ground truth0 None\n",
      "D:\\images\\tnia-python-images\\imagesc\\2025_11_30_segmenting_hollow_neurons\\patches\\tmpbkefvpq0 D:\\images\\tnia-python-images\\imagesc\\2025_11_30_segmenting_hollow_neurons\\patches\\ground truth0 None\n",
      "D:\\images\\tnia-python-images\\imagesc\\2025_11_30_segmenting_hollow_neurons\\patches\\tmpbkefvpq0 D:\\images\\tnia-python-images\\imagesc\\2025_11_30_segmenting_hollow_neurons\\patches\\ground truth0 None\n",
      "D:\\images\\tnia-python-images\\imagesc\\2025_11_30_segmenting_hollow_neurons\\patches\\tmpbkefvpq0 D:\\images\\tnia-python-images\\imagesc\\2025_11_30_segmenting_hollow_neurons\\patches\\ground truth0 None\n",
      "Training with device: cuda, model_type: vit_b_lm, n_objects_per_batch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verifying labels in 'train' dataloader: 100%|██████████| 50/50 [00:01<00:00, 26.81it/s]\n",
      "Verifying labels in 'val' dataloader: 100%|██████████| 50/50 [00:01<00:00, 28.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fitting for 1000 iterations /  5 epochs\n",
      "with 200 iterations per epoch\n",
      "Training with mixed precision\n",
      "Finished training after 5 epochs / 1000 iterations.\n",
      "The best epoch is number 4.\n",
      "Training took 1086.9265985488892 seconds (= 00:18:07 hours)\n",
      "Predicting using Micro-sam model\n",
      "Predicting using Micro-sam model\n"
     ]
    }
   ],
   "source": [
    "viewer = napari.Viewer()\n",
    "batch_dl = easy_augment_batch_dl.NapariEasyAugmentBatchDL(viewer, label_only = False)\n",
    "\n",
    "viewer.window.add_dock_widget(\n",
    "    batch_dl\n",
    ")\n",
    "\n",
    "parent_path = r'D:\\images\\tnia-python-images\\imagesc\\2025_11_30_segmenting_hollow_neurons'\n",
    "model_path = os.path.join(parent_path, 'models', 'checkpoints')\n",
    "framework_type = \"Micro-sam Instance Framework\"\n",
    "batch_dl.load_image_directory(parent_path)\n",
    "model_name = None #\"microsam_nov2025_3.5_vitb\"\n",
    "\n",
    "# optionally set a pretrained model and settings so we can do prediction\n",
    "batch_dl.network_architecture_drop_down.setCurrentText(framework_type)\n",
    "\n",
    "model = batch_dl.deep_learning_project.frameworks[framework_type]\n",
    "\n",
    "if model_name is not None:\n",
    "    model.model_name = model_name\n",
    "\n",
    "widget = batch_dl.deep_learning_widgets[framework_type]\n",
    "widget.sync_with_framework()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pixi (microsam_cellposesam)",
   "language": "python",
   "name": "microsam_cellposesam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
